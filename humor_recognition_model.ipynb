{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from googletrans import Translator\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import hashlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disk(embedding_filename, len_vec, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. and return the dictionary and/or list of embedding matrix\n",
    "    input: \n",
    "        glove_filename: str, the path of glove file\n",
    "        len_vec: int, a number to indicate the dimension of emedding layer\n",
    "        with_indexes: boolean, if it's True, it return a tuple of two dictionnaries \n",
    "                      (word_to_index_dict, index_to_embedding_array);\n",
    "                      otherwise, it return only a dictionary object, mapping from a string to a numpy array\n",
    "    \"\"\"\n",
    "    print(\"Loading embedding from disks...\")\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()    # key: word_string; value: index\n",
    "        index_to_embedding_array = []    # matrix, each row representes the embedding array of word with corresponding index\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()   # key: word; value: embedding array\n",
    "        \n",
    "    with open(embedding_filename, 'r', encoding='utf-8') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            # for each row, the values are separated by white space. The first element is word and followed by its embedding array\n",
    "            split = line.split(\" \")  \n",
    "            \n",
    "            # if we have length of split under a certain threshold, we discard the line\n",
    "            if len(split) < 5:\n",
    "                continue\n",
    "            \n",
    "            # if we have length of split more than give dimension, we cut it up to len_vec\n",
    "            # plus 1 because in split it has also a element that store the word\n",
    "            if len(split) > len_vec+1:\n",
    "                split = split[:len_vec+1]\n",
    "            \n",
    "            word = split[0]\n",
    "            representation = split[1:]\n",
    "            representation = np.array([float(val) for val in representation])\n",
    "            \n",
    "            # if we have len of dimension less than given number, we add the average list unless they have same length\n",
    "            if len(representation) < len_vec:\n",
    "                representation = np.append(representation, [np.mean(representation)]*(len_vec-len(representation)))\n",
    "                \n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representatihon\n",
    "                \n",
    "        _WORD_NOT_FOUND = [0.0] * len(representation)    # empty representation for unknow words\n",
    "        \n",
    "        if with_indexes:\n",
    "            _LAST_INDEX = i + 1\n",
    "            word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "            index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])  # any unknow word, it will find the last index of the this directory\n",
    "            print(\"Embedding loaded from disks. Return word-index dictionary and embedding matrix\")\n",
    "            return word_to_index_dict, index_to_embedding_array\n",
    "        else:\n",
    "            word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "            print(\"Embedding loaded from disks. Return word-embedding array dictionary\")\n",
    "            return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var_matrix_as_tf(matrix, ckpt_path, name='Embedding'):\n",
    "    \"\"\"\n",
    "    save the matrix of emebedded value as Tensorflow Variable. It can be used to load it directly from the next times\n",
    "    input:\n",
    "        matrix: list of lists, row, the embedded array of a word \n",
    "        ckpt_path: str, the path and the name of checkpoint file\n",
    "        name: str, the name of tensorflow variable that you want associate to \n",
    "    \"\"\"\n",
    "    print(\"saving the checkpoint file...\")\n",
    "    tf_embedding = tf.Variable(tf.constant(0.0, shape=matrix.shape),\n",
    "                               trainable=False,\n",
    "                               name=name)\n",
    "\n",
    "    # now, we will store embedding in Tensorflow\n",
    "    tf_embedding.assign(matrix)\n",
    "\n",
    "    embedding_saver = tf.saved_model.save(tf_embedding, export_dir=ckpt_path)\n",
    "    print(\"checkpoint file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(value, json_path):\n",
    "    \"\"\"\n",
    "    save the word-index dictionary into json format\n",
    "    input:\n",
    "        value, dict, with word as key and index number as value\n",
    "        json_path: str, the path and name of json file\n",
    "    \"\"\"\n",
    "    print(\"saving the json file...\")\n",
    "    if not os.path.exists(json_path):\n",
    "        os.makedirs(os.path.dirname(json_path))\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(value, f)\n",
    "        \n",
    "    print(\"json file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_word_to_index(dict_word_index_filename):\n",
    "    \"\"\"\n",
    "    Load a `word_to_index` dict mapping words to their id, with a default value\n",
    "    of pointing to the last index when not found, which is the unknown word.\n",
    "    input: \n",
    "        dict_word_index_filename: str, the filename and path to load the word-index dictionary\n",
    "    \"\"\"\n",
    "    with open(dict_word_index_filename, 'r') as f:\n",
    "        word_to_index = json.load(f)\n",
    "    _LAST_INDEX = len(word_to_index)\n",
    "    print(\"word_to_index dict restored from '{}'.\".format(dict_word_index_filename))\n",
    "    word_to_index = defaultdict(lambda: _LAST_INDEX, word_to_index)\n",
    "\n",
    "    return word_to_index\n",
    "\n",
    "def load_embedding_tf(tf_embeddings_file_path):\n",
    "    \"\"\"\n",
    "    load the embedding matrix from saved variable\n",
    "    \"\"\"\n",
    "    embedding_saver = tf.saved_model.load(export_dir=tf_embeddings_file_path)\n",
    "    print(\"TF embeddings restored from '{}'.\".format(tf_embeddings_file_path))\n",
    "    \n",
    "    return embedding_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_model_to_embedding_layer(pretrained_model, mod_dim, json_path, checkpoint_path):\n",
    "    \"\"\"\n",
    "    load the pre-trained model to the tensorflow embedding layer. \n",
    "    input:\n",
    "        pretrained_model: str, the path and the name of file contains the row with word and list of numbers\n",
    "        mod_dim: int, the number that indicates the dimensions (columns) of embedding matrix.\n",
    "        json_path: str, the path and name of json file, it has the structure \"word: index\"\n",
    "        checkpoint_path: str, the path and name of ckpt file, pre-trained model saved as tensorflow variable  \n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_index = None\n",
    "    index_to_embedding = None\n",
    "    \n",
    "    # if we have already the json file and the chechpoint file, then we can load directly from them\n",
    "    if os.path.exists(json_path) and os.path.exists(checkpoint_path):\n",
    "        print(\"load from json and checkpoint files\")\n",
    "        word_to_index = load_word_to_index(json_path)\n",
    "        index_to_embedding = load_embedding_tf(checkpoint_path).numpy()\n",
    "    else:\n",
    "        word_to_index, index_to_embedding = load_embedding_from_disk(pretrained_model, mod_dim, with_indexes=True)\n",
    "        save_json(word_to_index, json_path)\n",
    "        save_var_matrix_as_tf(index_to_embedding, ckpt_path=checkpoint_path)\n",
    "        \n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(len(index_to_embedding),\n",
    "                                                mod_dim,\n",
    "                                                weights=[index_to_embedding],\n",
    "                                                trainable=False)\n",
    "\n",
    "    \n",
    "    return embedding_layer, word_to_index, index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_sequences(list_sentences, word_index_dic, max_sequence_length):\n",
    "    sequence_matrix= np.array(list(map(lambda x: np.array([word_index_dic[i] for i in x]), list_sentences)))\n",
    "    data_padded = pad_sequences(sequence_matrix, maxlen=max_sequence_length)\n",
    "    \n",
    "    return sequence_matrix, data_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  model creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_filename, embedding_dim, embedding_json, embedding_ckpt, max_sequence_length, labels_index):\n",
    "    embedding_layer, word_to_index, index_to_embedding = pretrained_model_to_embedding_layer(embedding_filename, \n",
    "                                                                                             embedding_dim, \n",
    "                                                                                             embedding_json, \n",
    "                                                                                             embedding_ckpt)    \n",
    "\n",
    "    sequence_input = tf.keras.Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = tf.keras.layers.Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = tf.keras.layers.GlobalMaxPool1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = tf.keras.layers.concatenate(convs, axis=1)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.1)(l_merge)  \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    preds = tf.keras.layers.Dense(labels_index, activation='softmax')(x)    # sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = tf.keras.layers.MaxPooling1D(5)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    \"\"\"\n",
    "    model = tf.keras.Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove.txt\"\n",
    "embedding_ckpt = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var_checpoint\"\n",
    "embedding_json = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json\"\n",
    "\n",
    "checkpoint_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\"\n",
    "model_save_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/model_saved\"\n",
    "model_name = \"eng_joke.h5\"\n",
    "\n",
    "data_file_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/dataset/final/short_jokes.pickle\"\n",
    "\n",
    "embedding_dim = 50\n",
    "max_sequence_length = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405278</th>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>[martin, ellis, added, the, attraction, of, sh...</td>\n",
       "      <td>29</td>\n",
       "      <td>martin ellis added attraction shopping leisure...</td>\n",
       "      <td>[martin, ellis, added, attraction, shopping, l...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261267</th>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>0</td>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>[and, today, a, judge, in, state, supreme, cou...</td>\n",
       "      <td>28</td>\n",
       "      <td>today judge state supreme court staten island ...</td>\n",
       "      <td>[today, judge, state, supreme, court, staten, ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Joke  label  \\\n",
       "405278  martin ellis added the attraction of shopping ...      0   \n",
       "261267  and today a judge in state supreme court on st...      0   \n",
       "\n",
       "                                         Text_Clean_Punct  \\\n",
       "405278  martin ellis added the attraction of shopping ...   \n",
       "261267  and today a judge in state supreme court on st...   \n",
       "\n",
       "                                      Final_with_stopword  \\\n",
       "405278  martin ellis added the attraction of shopping ...   \n",
       "261267  and today a judge in state supreme court on st...   \n",
       "\n",
       "                                     tokens_with_stopword  \\\n",
       "405278  [martin, ellis, added, the, attraction, of, sh...   \n",
       "261267  [and, today, a, judge, in, state, supreme, cou...   \n",
       "\n",
       "        len_tokens_with_stopword  \\\n",
       "405278                        29   \n",
       "261267                        28   \n",
       "\n",
       "                                   Final_without_stopword  \\\n",
       "405278  martin ellis added attraction shopping leisure...   \n",
       "261267  today judge state supreme court staten island ...   \n",
       "\n",
       "                                  tokens_without_stopword  \\\n",
       "405278  [martin, ellis, added, attraction, shopping, l...   \n",
       "261267  [today, judge, state, supreme, court, staten, ...   \n",
       "\n",
       "        len_tokens_without_stopword  \n",
       "405278                           16  \n",
       "261267                           16  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(data_file_path)\n",
    "df = df.sample(frac=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split in train dataset and test dataset(using Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=0.1, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10540455 words total, with a vocabulary size of 85278\n",
      "Max sentence length is 268\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens_with_stopword\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens_with_stopword\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168836 words total, with a vocabulary size of 34989\n",
      "Max sentence length is 87\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens_with_stopword\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens_with_stopword\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from json and checkpoint files\n",
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json'.\n",
      "TF embeddings restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var_checpoint'.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).signatures\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 50)      20000050    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 199, 200)     20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 200)     30200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 200)     40200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 200)     50200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 195, 200)     60200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,329,436\n",
      "Trainable params: 329,386\n",
      "Non-trainable params: 20,000,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(glove_filename, embedding_dim, embedding_json, embedding_ckpt, max_sequence_length, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json'.\n"
     ]
    }
   ],
   "source": [
    "word_to_index = load_word_to_index(embedding_json)\n",
    "#index_to_embedding = load_embedding_tf(embedding_ckpt).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train, train_data_padded = create_pad_sequences(data_train[\"tokens_with_stopword\"].tolist(), word_to_index, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416982, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test, test_data_padded = create_pad_sequences(data_test[\"tokens_with_stopword\"].tolist(), word_to_index, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46332, 200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = [[1, 0]if v == 1 else [0, 1] for v in data_train['label'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416982, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array = np.array(label_array)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375283 samples, validate on 41699 samples\n",
      "Epoch 1/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9616\n",
      "Epoch 00001: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 772s 2ms/sample - loss: 0.1006 - acc: 0.9616 - val_loss: 0.0328 - val_acc: 0.9876\n",
      "Epoch 2/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9897\n",
      "Epoch 00002: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 717s 2ms/sample - loss: 0.0292 - acc: 0.9897 - val_loss: 0.0198 - val_acc: 0.9929\n",
      "Epoch 3/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9929\n",
      "Epoch 00003: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 716s 2ms/sample - loss: 0.0201 - acc: 0.9929 - val_loss: 0.0178 - val_acc: 0.9938\n",
      "Epoch 4/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9950\n",
      "Epoch 00004: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 714s 2ms/sample - loss: 0.0138 - acc: 0.9950 - val_loss: 0.0175 - val_acc: 0.9937\n",
      "Epoch 5/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9959\n",
      "Epoch 00005: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 714s 2ms/sample - loss: 0.0112 - acc: 0.9959 - val_loss: 0.0174 - val_acc: 0.9941\n",
      "Epoch 6/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9967\n",
      "Epoch 00006: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 712s 2ms/sample - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0201 - val_acc: 0.9939\n",
      "Epoch 7/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9973\n",
      "Epoch 00007: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 711s 2ms/sample - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0203 - val_acc: 0.9942\n",
      "Epoch 8/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9975\n",
      "Epoch 00008: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 710s 2ms/sample - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0197 - val_acc: 0.9939\n",
      "Epoch 9/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9979\n",
      "Epoch 00009: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 710s 2ms/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Epoch 10/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
      "Epoch 00010: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 708s 2ms/sample - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0209 - val_acc: 0.9940\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "hist = model.fit(train_data_padded, label_array, epochs=num_epoches, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "    \n",
    "model.save(os.path.join(model_save_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data_padded, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945178278511612"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156556</th>\n",
       "      <td>What tick likes to run? Politicks</td>\n",
       "      <td>1</td>\n",
       "      <td>What tick likes to run Politicks</td>\n",
       "      <td>what tick likes to run ? politicks</td>\n",
       "      <td>[what, tick, likes, to, run, ?, politicks]</td>\n",
       "      <td>7</td>\n",
       "      <td>tick likes run ? politicks</td>\n",
       "      <td>[tick, likes, run, ?, politicks]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19679</th>\n",
       "      <td>Instead of politely knocking on the bathroom d...</td>\n",
       "      <td>1</td>\n",
       "      <td>Instead of politely knocking on the bathroom d...</td>\n",
       "      <td>instead of politely knocking on the bathroom d...</td>\n",
       "      <td>[instead, of, politely, knocking, on, the, bat...</td>\n",
       "      <td>23</td>\n",
       "      <td>instead politely knocking bathroom door , kid ...</td>\n",
       "      <td>[instead, politely, knocking, bathroom, door, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136817</th>\n",
       "      <td>What's better than double-fisting a newborn? H...</td>\n",
       "      <td>1</td>\n",
       "      <td>Whats better than doublefisting a newborn HADO...</td>\n",
       "      <td>what 's better than double-fisting a newborn ?...</td>\n",
       "      <td>[what, 's, better, than, double-fisting, a, ne...</td>\n",
       "      <td>12</td>\n",
       "      <td>'s better double-fisting newborn ? hadouken ! ! !</td>\n",
       "      <td>['s, better, double-fisting, newborn, ?, hadou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182687</th>\n",
       "      <td>The American people should elect Gabe Newell p...</td>\n",
       "      <td>1</td>\n",
       "      <td>The American people should elect Gabe Newell p...</td>\n",
       "      <td>the american people should elect gabe newell p...</td>\n",
       "      <td>[the, american, people, should, elect, gabe, n...</td>\n",
       "      <td>32</td>\n",
       "      <td>american people elect gabe newell president 20...</td>\n",
       "      <td>[american, people, elect, gabe, newell, presid...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353616</th>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>0</td>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>[judge, marilyn, patel, of, the, us, district,...</td>\n",
       "      <td>31</td>\n",
       "      <td>judge marilyn patel us district court northern...</td>\n",
       "      <td>[judge, marilyn, patel, us, district, court, n...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Joke  label  \\\n",
       "156556                  What tick likes to run? Politicks      1   \n",
       "19679   Instead of politely knocking on the bathroom d...      1   \n",
       "136817  What's better than double-fisting a newborn? H...      1   \n",
       "182687  The American people should elect Gabe Newell p...      1   \n",
       "353616  judge marilyn patel of the us district court f...      0   \n",
       "\n",
       "                                         Text_Clean_Punct  \\\n",
       "156556                   What tick likes to run Politicks   \n",
       "19679   Instead of politely knocking on the bathroom d...   \n",
       "136817  Whats better than doublefisting a newborn HADO...   \n",
       "182687  The American people should elect Gabe Newell p...   \n",
       "353616  judge marilyn patel of the us district court f...   \n",
       "\n",
       "                                      Final_with_stopword  \\\n",
       "156556                 what tick likes to run ? politicks   \n",
       "19679   instead of politely knocking on the bathroom d...   \n",
       "136817  what 's better than double-fisting a newborn ?...   \n",
       "182687  the american people should elect gabe newell p...   \n",
       "353616  judge marilyn patel of the us district court f...   \n",
       "\n",
       "                                     tokens_with_stopword  \\\n",
       "156556         [what, tick, likes, to, run, ?, politicks]   \n",
       "19679   [instead, of, politely, knocking, on, the, bat...   \n",
       "136817  [what, 's, better, than, double-fisting, a, ne...   \n",
       "182687  [the, american, people, should, elect, gabe, n...   \n",
       "353616  [judge, marilyn, patel, of, the, us, district,...   \n",
       "\n",
       "        len_tokens_with_stopword  \\\n",
       "156556                         7   \n",
       "19679                         23   \n",
       "136817                        12   \n",
       "182687                        32   \n",
       "353616                        31   \n",
       "\n",
       "                                   Final_without_stopword  \\\n",
       "156556                         tick likes run ? politicks   \n",
       "19679   instead politely knocking bathroom door , kid ...   \n",
       "136817  's better double-fisting newborn ? hadouken ! ! !   \n",
       "182687  american people elect gabe newell president 20...   \n",
       "353616  judge marilyn patel us district court northern...   \n",
       "\n",
       "                                  tokens_without_stopword  \\\n",
       "156556                   [tick, likes, run, ?, politicks]   \n",
       "19679   [instead, politely, knocking, bathroom, door, ...   \n",
       "136817  ['s, better, double-fisting, newborn, ?, hadou...   \n",
       "182687  [american, people, elect, gabe, newell, presid...   \n",
       "353616  [judge, marilyn, patel, us, district, court, n...   \n",
       "\n",
       "        len_tokens_without_stopword  \n",
       "156556                            5  \n",
       "19679                            15  \n",
       "136817                            9  \n",
       "182687                           20  \n",
       "353616                           21  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test using other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 50)      20000050    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 199, 200)     20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 200)     30200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 200)     40200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 200)     50200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 195, 200)     60200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,329,436\n",
      "Trainable params: 329,386\n",
      "Non-trainable params: 20,000,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(os.path.join(model_save_path, model_name))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>Merkel says supports some kind of no-fly zone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Merkel says supports some kind of nofly zone i...</td>\n",
       "      <td>merkel says supports some kind of nofly zone i...</td>\n",
       "      <td>[merkel, says, supports, some, kind, of, nofly...</td>\n",
       "      <td>10</td>\n",
       "      <td>merkel says supports kind nofly zone syria</td>\n",
       "      <td>[merkel, says, supports, kind, nofly, zone, sy...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17609</th>\n",
       "      <td>They have two children , Shane and Stephanie ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>They have two children   Shane and Stephanie  ...</td>\n",
       "      <td>they have two children shane and stephanie who...</td>\n",
       "      <td>[they, have, two, children, shane, and, stepha...</td>\n",
       "      <td>12</td>\n",
       "      <td>two children shane stephanie work wwe</td>\n",
       "      <td>[two, children, shane, stephanie, work, wwe]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6811</th>\n",
       "      <td>I've taken up speed reading. I can read 'War a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Ive taken up speed reading  I can read War and...</td>\n",
       "      <td>ive taken up speed reading i can read war and ...</td>\n",
       "      <td>[ive, taken, up, speed, reading, i, can, read,...</td>\n",
       "      <td>22</td>\n",
       "      <td>ive taken speed reading read war peace 20 seco...</td>\n",
       "      <td>[ive, taken, speed, reading, read, war, peace,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Knock, Knock. Who's there? Francis. Francis wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>Knock  Knock  Whos there  Francis  Francis who...</td>\n",
       "      <td>knock knock whos there francis francis who fra...</td>\n",
       "      <td>[knock, knock, whos, there, francis, francis, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>knock knock whos francis francis francis next ...</td>\n",
       "      <td>[knock, knock, whos, francis, francis, francis...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11765</th>\n",
       "      <td>A year in space: Scott Kelly and Mikhail Korni...</td>\n",
       "      <td>0</td>\n",
       "      <td>A year in space Scott Kelly and Mikhail Kornie...</td>\n",
       "      <td>a year in space scott kelly and mikhail kornie...</td>\n",
       "      <td>[a, year, in, space, scott, kelly, and, mikhai...</td>\n",
       "      <td>12</td>\n",
       "      <td>year space scott kelly mikhail kornienko retur...</td>\n",
       "      <td>[year, space, scott, kelly, mikhail, kornienko...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  label  \\\n",
       "16628  Merkel says supports some kind of no-fly zone ...      0   \n",
       "17609  They have two children , Shane and Stephanie ,...      0   \n",
       "6811   I've taken up speed reading. I can read 'War a...      1   \n",
       "1092   Knock, Knock. Who's there? Francis. Francis wh...      1   \n",
       "11765  A year in space: Scott Kelly and Mikhail Korni...      0   \n",
       "\n",
       "                                        Text_Clean_Punct  \\\n",
       "16628  Merkel says supports some kind of nofly zone i...   \n",
       "17609  They have two children   Shane and Stephanie  ...   \n",
       "6811   Ive taken up speed reading  I can read War and...   \n",
       "1092   Knock  Knock  Whos there  Francis  Francis who...   \n",
       "11765  A year in space Scott Kelly and Mikhail Kornie...   \n",
       "\n",
       "                                     Final_with_stopword  \\\n",
       "16628  merkel says supports some kind of nofly zone i...   \n",
       "17609  they have two children shane and stephanie who...   \n",
       "6811   ive taken up speed reading i can read war and ...   \n",
       "1092   knock knock whos there francis francis who fra...   \n",
       "11765  a year in space scott kelly and mikhail kornie...   \n",
       "\n",
       "                                    tokens_with_stopword  \\\n",
       "16628  [merkel, says, supports, some, kind, of, nofly...   \n",
       "17609  [they, have, two, children, shane, and, stepha...   \n",
       "6811   [ive, taken, up, speed, reading, i, can, read,...   \n",
       "1092   [knock, knock, whos, there, francis, francis, ...   \n",
       "11765  [a, year, in, space, scott, kelly, and, mikhai...   \n",
       "\n",
       "       len_tokens_with_stopword  \\\n",
       "16628                        10   \n",
       "17609                        12   \n",
       "6811                         22   \n",
       "1092                         11   \n",
       "11765                        12   \n",
       "\n",
       "                                  Final_without_stopword  \\\n",
       "16628         merkel says supports kind nofly zone syria   \n",
       "17609              two children shane stephanie work wwe   \n",
       "6811   ive taken speed reading read war peace 20 seco...   \n",
       "1092   knock knock whos francis francis francis next ...   \n",
       "11765  year space scott kelly mikhail kornienko retur...   \n",
       "\n",
       "                                 tokens_without_stopword  \\\n",
       "16628  [merkel, says, supports, kind, nofly, zone, sy...   \n",
       "17609       [two, children, shane, stephanie, work, wwe]   \n",
       "6811   [ive, taken, speed, reading, read, war, peace,...   \n",
       "1092   [knock, knock, whos, francis, francis, francis...   \n",
       "11765  [year, space, scott, kelly, mikhail, kornienko...   \n",
       "\n",
       "       len_tokens_without_stopword  \n",
       "16628                            7  \n",
       "17609                            6  \n",
       "6811                            12  \n",
       "1092                             8  \n",
       "11765                            8  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filename = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/dataset/final/shorttext.pickle\"\n",
    "data_out = pd.read_pickle(test_filename)\n",
    "data_out = data_out.sample(frac=1)\n",
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_pad_sequences() missing 1 required positional argument: 'max_sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c9407f0950dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2_data_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens_with_stopword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: create_pad_sequences() missing 1 required positional argument: 'max_sequence_length'"
     ]
    }
   ],
   "source": [
    "seq_test2, test2_data_padded = create_pad_sequences(data_out[\"tokens_with_stopword\"].tolist(), word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test2 = model.predict(test2_data_padded, batch_size=1024, verbose=0)\n",
    "\n",
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions_test2:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "sum(data_out.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinse part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tencent = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt\"\n",
    "embedding_ckpt_ch = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/tencent/var.ckpt\"\n",
    "embedding_json_ch = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/tencent/var.json\"\n",
    "\n",
    "checkpoint_path_ch = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/ch_nlp/checkpoint/\"\n",
    "model_save_path_ch = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/ch_nlp/model_saved\"\n",
    "model_name_ch = \"chn_joke.h5\"\n",
    "\n",
    "embedding_dim_ch = 200\n",
    "max_sequence_len_ch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dadf3052bcf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mch_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtencent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_json_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_ckpt_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "ch_model = create_model(tencent, embedding_dim_ch, embedding_json_ch, embedding_ckpt_ch, max_sequence_len_ch, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67125</th>\n",
       "      <td>2          1.77                  1.8%   ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2          177                  18      ...</td>\n",
       "      <td>[, , , , 2, ,  , ,  , ,  , ,  , ,  , ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43595</th>\n",
       "      <td>8:45</td>\n",
       "      <td>1</td>\n",
       "      <td>845</td>\n",
       "      <td>[8, , , , , , , 45, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34629</th>\n",
       "      <td> </td>\n",
       "      <td>1</td>\n",
       "      <td> </td>\n",
       "      <td>[, , , , , , , , ,  , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61016</th>\n",
       "      <td>307512200...</td>\n",
       "      <td>0</td>\n",
       "      <td>307512200...</td>\n",
       "      <td>[, , , , , 3075, , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16589</th>\n",
       "      <td>,,</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>[, , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29244</th>\n",
       "      <td>78...</td>\n",
       "      <td>1</td>\n",
       "      <td>78...</td>\n",
       "      <td>[, , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31041</th>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "      <td>~</td>\n",
       "      <td>[, , ~, , , , , , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108019</th>\n",
       "      <td>522...</td>\n",
       "      <td>0</td>\n",
       "      <td>522...</td>\n",
       "      <td>[, , , , , , , , , , , ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64640</th>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[, , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100358</th>\n",
       "      <td>12%...</td>\n",
       "      <td>0</td>\n",
       "      <td>12...</td>\n",
       "      <td>[, , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111614 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentences  label  \\\n",
       "67125   2          1.77                  1.8%   ...      0   \n",
       "43595                                    8:45      1   \n",
       "34629                   1   \n",
       "61016   307512200...      0   \n",
       "16589                    ,,      1   \n",
       "...                                                   ...    ...   \n",
       "29244   78...      1   \n",
       "31041                                     ~      1   \n",
       "108019  522...      0   \n",
       "64640   ...      0   \n",
       "100358  12%...      0   \n",
       "\n",
       "                                                  no_punc  \\\n",
       "67125   2          177                  18      ...   \n",
       "43595                                      845   \n",
       "34629                         \n",
       "61016   307512200...   \n",
       "16589                          \n",
       "...                                                   ...   \n",
       "29244   78...   \n",
       "31041                                       ~   \n",
       "108019  522...   \n",
       "64640   ...   \n",
       "100358  12...   \n",
       "\n",
       "                                                   tokens  \n",
       "67125   [, , , , 2, ,  , ,  , ,  , ,  , ,  , ,...  \n",
       "43595                   [8, , , , , , , 45, ]  \n",
       "34629   [, , , , , , , , ,  , , , , , ...  \n",
       "61016   [, , , , , 3075, , , , , ...  \n",
       "16589   [, , , , , , , , , , , , , ...  \n",
       "...                                                   ...  \n",
       "29244   [, , , , , , , , , , , , , ...  \n",
       "31041                    [, , ~, , , , , , ]  \n",
       "108019  [, , , , , , , , , , , ,...  \n",
       "64640   [, , , , , , , , , , , ...  \n",
       "100358  [, , , , , , , , , , , , , ...  \n",
       "\n",
       "[111614 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ch_dataset = pd.read_pickle(\"./data_created/chinese_dataset.pickle\")\n",
    "df_ch_dataset = df_ch_dataset.sample(frac=1)\n",
    "df_ch_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(df_ch_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from 'C:\\Users\\zhany\\Documents\\learning\\humor_recognition\\data\\emb_pretrained\\chn\\checpoint\\tencent\\var.json'.\n",
      "TF embeddings restored from 'C:\\Users\\zhany\\Documents\\learning\\humor_recognition\\data\\emb_pretrained\\chn\\checpoint\\tencent\\var.ckpt'.\n"
     ]
    }
   ],
   "source": [
    "word_to_index_ch = load_word_to_index(embedding_json_ch)\n",
    "#index_to_embedding_ch = load_embedding_tf(embedding_ckpt_ch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89291, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_train_ch, train_data_padded_ch = create_pad_sequences(train_dataset[\"tokens\"].tolist(), word_to_index_ch, max_sequence_len_ch)\n",
    "train_data_padded_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22323, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_test_ch, test_data_padded_ch = create_pad_sequences(test_dataset[\"tokens\"].tolist(), word_to_index_ch, max_sequence_len_ch)\n",
    "test_data_padded_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89291, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array_ch = [[1, 0]if v == 1 else [0, 1] for v in train_dataset['label'].values]\n",
    "label_array_ch = np.array(label_array_ch)\n",
    "label_array_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80361 samples, validate on 8930 samples\n",
      "Epoch 1/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.4729 - acc: 0.7840 \n",
      "Epoch 00001: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1166s 15ms/sample - loss: 0.4714 - acc: 0.7847 - val_loss: 0.2245 - val_acc: 0.9137\n",
      "Epoch 2/10\n",
      "79872/80361 [============================>.] - ETA: 9s - loss: 0.1871 - acc: 0.9305 \n",
      "Epoch 00002: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1676s 21ms/sample - loss: 0.1868 - acc: 0.9307 - val_loss: 0.1442 - val_acc: 0.9434\n",
      "Epoch 3/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.1071 - acc: 0.9627 \n",
      "Epoch 00003: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1230s 15ms/sample - loss: 0.1071 - acc: 0.9627 - val_loss: 0.1078 - val_acc: 0.9598\n",
      "Epoch 4/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0571 - acc: 0.9827 \n",
      "Epoch 00004: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1098s 14ms/sample - loss: 0.0570 - acc: 0.9828 - val_loss: 0.1055 - val_acc: 0.9600\n",
      "Epoch 5/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0357 - acc: 0.9895 \n",
      "Epoch 00005: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1108s 14ms/sample - loss: 0.0356 - acc: 0.9895 - val_loss: 0.1180 - val_acc: 0.9597\n",
      "Epoch 6/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0184 - acc: 0.9955 \n",
      "Epoch 00006: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1169s 15ms/sample - loss: 0.0183 - acc: 0.9956 - val_loss: 0.1038 - val_acc: 0.9641\n",
      "Epoch 7/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0110 - acc: 0.9977 \n",
      "Epoch 00007: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1200s 15ms/sample - loss: 0.0110 - acc: 0.9977 - val_loss: 0.1168 - val_acc: 0.9644\n",
      "Epoch 8/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0066 - acc: 0.9989 \n",
      "Epoch 00008: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1153s 14ms/sample - loss: 0.0066 - acc: 0.9990 - val_loss: 0.1178 - val_acc: 0.9656\n",
      "Epoch 9/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0046 - acc: 0.9992 \n",
      "Epoch 00009: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1159s 14ms/sample - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1161 - val_acc: 0.9660\n",
      "Epoch 10/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0045 - acc: 0.9992 \n",
      "Epoch 00010: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1057s 13ms/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1163 - val_acc: 0.9673\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(checkpoint_path_ch):\n",
    "    os.makedirs(checkpoint_path_ch)\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_ch, save_weights_only=True, verbose=1)\n",
    "hist = ch_model.fit(train_data_padded_ch, label_array_ch, epochs=num_epoches, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_save_path_ch):\n",
    "    os.makedirs(model_save_path_ch)\n",
    "    \n",
    "ch_model.save(os.path.join(model_save_path_ch, model_name_ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d62d463e213f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_padded_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ch_model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = ch_model.predict(test_data_padded_ch, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9657304125789544"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "sum(test_dataset.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict the eastern or/and western joke (DA QUI POSSO LANCIARE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_translate_eng_cn(sentence):\n",
    "    url = 'http://api.fanyi.baidu.com/api/trans/vip/translate'\n",
    "\n",
    "    # non mostrare appid e key agli altri\n",
    "    appid = '20200329000408142'\n",
    "    key = 'riUpq41_ifBFCFB5c6NF'\n",
    "\n",
    "    salt = '12345654321234'   #  random number\n",
    "    \n",
    "    sign = hashlib.md5((appid + sentence + salt + key).encode('UTF-8')).hexdigest()\n",
    "    \n",
    "    en_form_data = {\n",
    "    'q': sentence,         \n",
    "    'from': 'auto',    \n",
    "    'to': \"en\",   \n",
    "    'appid': appid,\n",
    "    'salt': salt,\n",
    "    'sign': sign\n",
    "    }\n",
    "    en_request = requests.get(url, params=en_form_data)\n",
    "    en_sentence = en_request.json()['trans_result'][0]['dst']\n",
    "    \n",
    "    time.sleep(1)  # pausa 1 sec, standard baidu api limitation\n",
    "    \n",
    "    cn_form_data = {\n",
    "        'q': sentence,\n",
    "        'from': 'auto',\n",
    "        'to': 'zh',\n",
    "        'appid': appid,\n",
    "        'salt': salt,\n",
    "        'sign': sign\n",
    "    }\n",
    "    cn_request = requests.get(url, params=cn_form_data)\n",
    "    cn_sentence = cn_request.json()['trans_result'][0]['dst']\n",
    "    \n",
    "    return en_sentence, cn_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Try Baidu translation', '')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baidu_translate_eng_cn(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_translate_eng_cn(sentence):\n",
    "    \"\"\"\n",
    "    return the englesh and chinese sentence by using google translate\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    eng_sentence = translator.translate(sentence, dest=\"en\").text\n",
    "    cn_sentence = translator.translate(sentence, dest=\"zh-cn\").text\n",
    "    return eng_sentence, cn_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('try google translator', '')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_translate_eng_cn(\"try google translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_word_eng(sentence):\n",
    "    \"\"\"\n",
    "    tokenizing the input sentence, include also remove punctuation, lower casing\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return [word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentence, model, word_to_index, max_seq_length):\n",
    "    _, sentence_padded = create_pad_sequences(tokenizing_word_eng(sentence), word_to_index, max_seq_length)\n",
    "    result = model.predict(sentence_padded)\n",
    "    \n",
    "    label = [1, 0]\n",
    "    prediction = label[np.argmax(result)]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(translator=\"google\"):\n",
    "    \n",
    "    en_word_json_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json\"\n",
    "    cn_word_json_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/cn_var.json\"\n",
    "    en_model_path = \"../model/modelli_creati/eng_joke.h5\"\n",
    "    cn_model_path = \"../model/modelli_creati/chn_joke.h5\"\n",
    "\n",
    "    en_seq_len = 200\n",
    "    cn_seq_len = 200\n",
    "    \n",
    "    en_word_to_index = load_word_to_index(en_word_json_path)\n",
    "   # cn_word_to_index = load_word_to_index(cn_word_json_path)\n",
    "    \n",
    "    en_model = tf.keras.models.load_model(en_model_path)\n",
    "   # cn_model = tf.keras.models.load_model(cn_model_path)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            sentence = str(input(\"Please insert your sentence: \"))\n",
    "            if sentence.lower().strip() == \"end\":\n",
    "                break\n",
    "            if translator.lower().strip() == \"google\":\n",
    "                en_sentence, cn_sentence = google_translate_eng_cn(sentence)\n",
    "            elif translator.lower().strip() == \"baidu\":\n",
    "                en_sentence, cn_sentence = baidu_translate_eng_cn(sentence)\n",
    "            else:\n",
    "                print(\"there are not translator requested\")\n",
    "            en_result = prediction(en_sentence, en_model, en_word_to_index, en_seq_len)\n",
    "           # cn_result = prediction(cn_sentence, cn_model, cn_word_to_index,cn_seq_len)\n",
    "            \n",
    "            if en_result == 1:\n",
    "                print(\"this is an english joke\")\n",
    "            else:\n",
    "                print(\"this is not an english joke\")\n",
    "          #  if cn_result == 1:\n",
    "           #     print(\"this is a chinese joke\")\n",
    "           # else:\n",
    "          #      print(\"this is not a chinese joke\")\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'.\n",
      "Please insert your sentence: hi how are you\n",
      "this is an english joke\n",
      "Please insert your sentence: My mother-in-law fell down a wishing well. I was amazed  I never knew they worked\n",
      "this is an english joke\n",
      "Please insert your sentence: i am 23 years old\n",
      "this is an english joke\n",
      "Please insert your sentence:  MediaWiki helps you collect and organize knowledge and make it available to people.\n",
      "this is an english joke\n",
      "Please insert your sentence: God wants spiritual fruit, not religious nuts.\n",
      "this is an english joke\n",
      "Please insert your sentence: That day, when they came back from school, their own son said\n",
      "this is not an english joke\n"
     ]
    }
   ],
   "source": [
    "main(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2c223e39d42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#funzione per testare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-87d519270f40>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(translator)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcn_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0men_word_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_word_json_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0;31m# cn_word_to_index = load_word_to_index(cn_word_json_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-735cf9ef291b>\u001b[0m in \u001b[0;36mload_word_to_index\u001b[0;34m(dict_word_index_filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdict_word_index_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindex\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_word_index_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_LAST_INDEX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'"
     ]
    }
   ],
   "source": [
    "main(\"google\") #funzione per testare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
