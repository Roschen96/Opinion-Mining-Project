{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from googletrans import Translator\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import hashlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disk(embedding_filename, len_vec, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. and return the dictionary and/or list of embedding matrix\n",
    "    input: \n",
    "        glove_filename: str, the path of glove file\n",
    "        len_vec: int, a number to indicate the dimension of emedding layer\n",
    "        with_indexes: boolean, if it's True, it return a tuple of two dictionnaries \n",
    "                      (word_to_index_dict, index_to_embedding_array);\n",
    "                      otherwise, it return only a dictionary object, mapping from a string to a numpy array\n",
    "    \"\"\"\n",
    "    print(\"Loading embedding from disks...\")\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()    # key: word_string; value: index\n",
    "        index_to_embedding_array = []    # matrix, each row representes the embedding array of word with corresponding index\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()   # key: word; value: embedding array\n",
    "        \n",
    "    with open(embedding_filename, 'r', encoding='utf-8') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            # for each row, the values are separated by white space. The first element is word and followed by its embedding array\n",
    "            split = line.split(\" \")  \n",
    "            \n",
    "            # if we have length of split under a certain threshold, we discard the line\n",
    "            if len(split) < 5:\n",
    "                continue\n",
    "            \n",
    "            # if we have length of split more than give dimension, we cut it up to len_vec\n",
    "            # plus 1 because in split it has also a element that store the word\n",
    "            if len(split) > len_vec+1:\n",
    "                split = split[:len_vec+1]\n",
    "            \n",
    "            word = split[0]\n",
    "            representation = split[1:]\n",
    "            representation = np.array([float(val) for val in representation])\n",
    "            \n",
    "            # if we have len of dimension less than given number, we add the average list unless they have same length\n",
    "            if len(representation) < len_vec:\n",
    "                representation = np.append(representation, [np.mean(representation)]*(len_vec-len(representation)))\n",
    "                \n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representatihon\n",
    "                \n",
    "        _WORD_NOT_FOUND = [0.0] * len(representation)    # empty representation for unknow words\n",
    "        \n",
    "        if with_indexes:\n",
    "            _LAST_INDEX = i + 1\n",
    "            word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "            index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])  # any unknow word, it will find the last index of the this directory\n",
    "            print(\"Embedding loaded from disks. Return word-index dictionary and embedding matrix\")\n",
    "            return word_to_index_dict, index_to_embedding_array\n",
    "        else:\n",
    "            word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "            print(\"Embedding loaded from disks. Return word-embedding array dictionary\")\n",
    "            return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var_matrix_as_tf(matrix, ckpt_path, name='Embedding'):\n",
    "    \"\"\"\n",
    "    save the matrix of emebedded value as Tensorflow Variable. It can be used to load it directly from the next times\n",
    "    input:\n",
    "        matrix: list of lists, row, the embedded array of a word \n",
    "        ckpt_path: str, the path and the name of checkpoint file\n",
    "        name: str, the name of tensorflow variable that you want associate to \n",
    "    \"\"\"\n",
    "    print(\"saving the checkpoint file...\")\n",
    "    tf_embedding = tf.Variable(tf.constant(0.0, shape=matrix.shape),\n",
    "                               trainable=False,\n",
    "                               name=name)\n",
    "\n",
    "    # now, we will store embedding in Tensorflow\n",
    "    tf_embedding.assign(matrix)\n",
    "\n",
    "    embedding_saver = tf.saved_model.save(tf_embedding, export_dir=ckpt_path)\n",
    "    print(\"checkpoint file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(value, json_path):\n",
    "    \"\"\"\n",
    "    save the word-index dictionary into json format\n",
    "    input:\n",
    "        value, dict, with word as key and index number as value\n",
    "        json_path: str, the path and name of json file\n",
    "    \"\"\"\n",
    "    print(\"saving the json file...\")\n",
    "    if not os.path.exists(json_path):\n",
    "        os.makedirs(os.path.dirname(json_path))\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(value, f)\n",
    "        \n",
    "    print(\"json file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_word_to_index(dict_word_index_filename):\n",
    "    \"\"\"\n",
    "    Load a `word_to_index` dict mapping words to their id, with a default value\n",
    "    of pointing to the last index when not found, which is the unknown word.\n",
    "    input: \n",
    "        dict_word_index_filename: str, the filename and path to load the word-index dictionary\n",
    "    \"\"\"\n",
    "    with open(dict_word_index_filename, 'r') as f:\n",
    "        word_to_index = json.load(f)\n",
    "    _LAST_INDEX = len(word_to_index)\n",
    "    print(\"word_to_index dict restored from '{}'.\".format(dict_word_index_filename))\n",
    "    word_to_index = defaultdict(lambda: _LAST_INDEX, word_to_index)\n",
    "\n",
    "    return word_to_index\n",
    "\n",
    "def load_embedding_tf(tf_embeddings_file_path):\n",
    "    \"\"\"\n",
    "    load the embedding matrix from saved variable\n",
    "    \"\"\"\n",
    "    embedding_saver = tf.saved_model.load(export_dir=tf_embeddings_file_path)\n",
    "    print(\"TF embeddings restored from '{}'.\".format(tf_embeddings_file_path))\n",
    "    \n",
    "    return embedding_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_model_to_embedding_layer(pretrained_model, mod_dim, json_path, checkpoint_path):\n",
    "    \"\"\"\n",
    "    load the pre-trained model to the tensorflow embedding layer. \n",
    "    input:\n",
    "        pretrained_model: str, the path and the name of file contains the row with word and list of numbers\n",
    "        mod_dim: int, the number that indicates the dimensions (columns) of embedding matrix.\n",
    "        json_path: str, the path and name of json file, it has the structure \"word: index\"\n",
    "        checkpoint_path: str, the path and name of ckpt file, pre-trained model saved as tensorflow variable  \n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_index = None\n",
    "    index_to_embedding = None\n",
    "    \n",
    "    # if we have already the json file and the chechpoint file, then we can load directly from them\n",
    "    if os.path.exists(json_path) and os.path.exists(checkpoint_path):\n",
    "        print(\"load from json and checkpoint files\")\n",
    "        word_to_index = load_word_to_index(json_path)\n",
    "        index_to_embedding = load_embedding_tf(checkpoint_path).numpy()\n",
    "    else:\n",
    "        word_to_index, index_to_embedding = load_embedding_from_disk(pretrained_model, mod_dim, with_indexes=True)\n",
    "        save_json(word_to_index, json_path)\n",
    "        save_var_matrix_as_tf(index_to_embedding, ckpt_path=checkpoint_path)\n",
    "        \n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(len(index_to_embedding),\n",
    "                                                mod_dim,\n",
    "                                                weights=[index_to_embedding],\n",
    "                                                trainable=False)\n",
    "\n",
    "    \n",
    "    return embedding_layer, word_to_index, index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_sequences(list_sentences, word_index_dic, max_sequence_length):\n",
    "    sequence_matrix= np.array(list(map(lambda x: np.array([word_index_dic[i] for i in x]), list_sentences)))\n",
    "    data_padded = pad_sequences(sequence_matrix, maxlen=max_sequence_length)\n",
    "    \n",
    "    return sequence_matrix, data_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  model creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_filename, embedding_dim, embedding_json, embedding_ckpt, max_sequence_length, labels_index):\n",
    "    embedding_layer, word_to_index, index_to_embedding = pretrained_model_to_embedding_layer(embedding_filename, \n",
    "                                                                                             embedding_dim, \n",
    "                                                                                             embedding_json, \n",
    "                                                                                             embedding_ckpt)    \n",
    "\n",
    "    sequence_input = tf.keras.Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = tf.keras.layers.Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = tf.keras.layers.GlobalMaxPool1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = tf.keras.layers.concatenate(convs, axis=1)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.1)(l_merge)  \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    preds = tf.keras.layers.Dense(labels_index, activation='softmax')(x)    # sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = tf.keras.layers.MaxPooling1D(5)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    \"\"\"\n",
    "    model = tf.keras.Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove.txt\"\n",
    "embedding_ckpt = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var_checpoint\"\n",
    "embedding_json = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json\"\n",
    "\n",
    "checkpoint_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\"\n",
    "model_save_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/model_saved\"\n",
    "model_name = \"eng_joke.h5\"\n",
    "\n",
    "data_file_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/dataset/final/short_jokes.pickle\"\n",
    "\n",
    "embedding_dim = 50\n",
    "max_sequence_length = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405278</th>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>martin ellis added the attraction of shopping ...</td>\n",
       "      <td>[martin, ellis, added, the, attraction, of, sh...</td>\n",
       "      <td>29</td>\n",
       "      <td>martin ellis added attraction shopping leisure...</td>\n",
       "      <td>[martin, ellis, added, attraction, shopping, l...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261267</th>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>0</td>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>and today a judge in state supreme court on st...</td>\n",
       "      <td>[and, today, a, judge, in, state, supreme, cou...</td>\n",
       "      <td>28</td>\n",
       "      <td>today judge state supreme court staten island ...</td>\n",
       "      <td>[today, judge, state, supreme, court, staten, ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Joke  label  \\\n",
       "405278  martin ellis added the attraction of shopping ...      0   \n",
       "261267  and today a judge in state supreme court on st...      0   \n",
       "\n",
       "                                         Text_Clean_Punct  \\\n",
       "405278  martin ellis added the attraction of shopping ...   \n",
       "261267  and today a judge in state supreme court on st...   \n",
       "\n",
       "                                      Final_with_stopword  \\\n",
       "405278  martin ellis added the attraction of shopping ...   \n",
       "261267  and today a judge in state supreme court on st...   \n",
       "\n",
       "                                     tokens_with_stopword  \\\n",
       "405278  [martin, ellis, added, the, attraction, of, sh...   \n",
       "261267  [and, today, a, judge, in, state, supreme, cou...   \n",
       "\n",
       "        len_tokens_with_stopword  \\\n",
       "405278                        29   \n",
       "261267                        28   \n",
       "\n",
       "                                   Final_without_stopword  \\\n",
       "405278  martin ellis added attraction shopping leisure...   \n",
       "261267  today judge state supreme court staten island ...   \n",
       "\n",
       "                                  tokens_without_stopword  \\\n",
       "405278  [martin, ellis, added, attraction, shopping, l...   \n",
       "261267  [today, judge, state, supreme, court, staten, ...   \n",
       "\n",
       "        len_tokens_without_stopword  \n",
       "405278                           16  \n",
       "261267                           16  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(data_file_path)\n",
    "df = df.sample(frac=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split in train dataset and test dataset(using Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=0.1, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10540455 words total, with a vocabulary size of 85278\n",
      "Max sentence length is 268\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens_with_stopword\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens_with_stopword\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168836 words total, with a vocabulary size of 34989\n",
      "Max sentence length is 87\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens_with_stopword\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens_with_stopword\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from json and checkpoint files\n",
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json'.\n",
      "TF embeddings restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var_checpoint'.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).signatures\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 50)      20000050    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 199, 200)     20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 200)     30200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 200)     40200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 200)     50200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 195, 200)     60200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,329,436\n",
      "Trainable params: 329,386\n",
      "Non-trainable params: 20,000,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(glove_filename, embedding_dim, embedding_json, embedding_ckpt, max_sequence_length, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/glove.6B/glove/var.json'.\n"
     ]
    }
   ],
   "source": [
    "word_to_index = load_word_to_index(embedding_json)\n",
    "#index_to_embedding = load_embedding_tf(embedding_ckpt).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train, train_data_padded = create_pad_sequences(data_train[\"tokens_with_stopword\"].tolist(), word_to_index, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416982, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test, test_data_padded = create_pad_sequences(data_test[\"tokens_with_stopword\"].tolist(), word_to_index, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46332, 200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = [[1, 0]if v == 1 else [0, 1] for v in data_train['label'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416982, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array = np.array(label_array)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375283 samples, validate on 41699 samples\n",
      "Epoch 1/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9616\n",
      "Epoch 00001: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 772s 2ms/sample - loss: 0.1006 - acc: 0.9616 - val_loss: 0.0328 - val_acc: 0.9876\n",
      "Epoch 2/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9897\n",
      "Epoch 00002: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 717s 2ms/sample - loss: 0.0292 - acc: 0.9897 - val_loss: 0.0198 - val_acc: 0.9929\n",
      "Epoch 3/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9929\n",
      "Epoch 00003: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 716s 2ms/sample - loss: 0.0201 - acc: 0.9929 - val_loss: 0.0178 - val_acc: 0.9938\n",
      "Epoch 4/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9950\n",
      "Epoch 00004: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 714s 2ms/sample - loss: 0.0138 - acc: 0.9950 - val_loss: 0.0175 - val_acc: 0.9937\n",
      "Epoch 5/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9959\n",
      "Epoch 00005: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 714s 2ms/sample - loss: 0.0112 - acc: 0.9959 - val_loss: 0.0174 - val_acc: 0.9941\n",
      "Epoch 6/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9967\n",
      "Epoch 00006: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 712s 2ms/sample - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0201 - val_acc: 0.9939\n",
      "Epoch 7/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9973\n",
      "Epoch 00007: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 711s 2ms/sample - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0203 - val_acc: 0.9942\n",
      "Epoch 8/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9975\n",
      "Epoch 00008: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 710s 2ms/sample - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0197 - val_acc: 0.9939\n",
      "Epoch 9/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9979\n",
      "Epoch 00009: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 710s 2ms/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Epoch 10/10\n",
      "374784/375283 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
      "Epoch 00010: saving model to /Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/eng_nlp/checkpoint/\n",
      "375283/375283 [==============================] - 708s 2ms/sample - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0209 - val_acc: 0.9940\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "hist = model.fit(train_data_padded, label_array, epochs=num_epoches, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "    \n",
    "model.save(os.path.join(model_save_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data_padded, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945178278511612"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156556</th>\n",
       "      <td>What tick likes to run? Politicks</td>\n",
       "      <td>1</td>\n",
       "      <td>What tick likes to run Politicks</td>\n",
       "      <td>what tick likes to run ? politicks</td>\n",
       "      <td>[what, tick, likes, to, run, ?, politicks]</td>\n",
       "      <td>7</td>\n",
       "      <td>tick likes run ? politicks</td>\n",
       "      <td>[tick, likes, run, ?, politicks]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19679</th>\n",
       "      <td>Instead of politely knocking on the bathroom d...</td>\n",
       "      <td>1</td>\n",
       "      <td>Instead of politely knocking on the bathroom d...</td>\n",
       "      <td>instead of politely knocking on the bathroom d...</td>\n",
       "      <td>[instead, of, politely, knocking, on, the, bat...</td>\n",
       "      <td>23</td>\n",
       "      <td>instead politely knocking bathroom door , kid ...</td>\n",
       "      <td>[instead, politely, knocking, bathroom, door, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136817</th>\n",
       "      <td>What's better than double-fisting a newborn? H...</td>\n",
       "      <td>1</td>\n",
       "      <td>Whats better than doublefisting a newborn HADO...</td>\n",
       "      <td>what 's better than double-fisting a newborn ?...</td>\n",
       "      <td>[what, 's, better, than, double-fisting, a, ne...</td>\n",
       "      <td>12</td>\n",
       "      <td>'s better double-fisting newborn ? hadouken ! ! !</td>\n",
       "      <td>['s, better, double-fisting, newborn, ?, hadou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182687</th>\n",
       "      <td>The American people should elect Gabe Newell p...</td>\n",
       "      <td>1</td>\n",
       "      <td>The American people should elect Gabe Newell p...</td>\n",
       "      <td>the american people should elect gabe newell p...</td>\n",
       "      <td>[the, american, people, should, elect, gabe, n...</td>\n",
       "      <td>32</td>\n",
       "      <td>american people elect gabe newell president 20...</td>\n",
       "      <td>[american, people, elect, gabe, newell, presid...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353616</th>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>0</td>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>judge marilyn patel of the us district court f...</td>\n",
       "      <td>[judge, marilyn, patel, of, the, us, district,...</td>\n",
       "      <td>31</td>\n",
       "      <td>judge marilyn patel us district court northern...</td>\n",
       "      <td>[judge, marilyn, patel, us, district, court, n...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Joke  label  \\\n",
       "156556                  What tick likes to run? Politicks      1   \n",
       "19679   Instead of politely knocking on the bathroom d...      1   \n",
       "136817  What's better than double-fisting a newborn? H...      1   \n",
       "182687  The American people should elect Gabe Newell p...      1   \n",
       "353616  judge marilyn patel of the us district court f...      0   \n",
       "\n",
       "                                         Text_Clean_Punct  \\\n",
       "156556                   What tick likes to run Politicks   \n",
       "19679   Instead of politely knocking on the bathroom d...   \n",
       "136817  Whats better than doublefisting a newborn HADO...   \n",
       "182687  The American people should elect Gabe Newell p...   \n",
       "353616  judge marilyn patel of the us district court f...   \n",
       "\n",
       "                                      Final_with_stopword  \\\n",
       "156556                 what tick likes to run ? politicks   \n",
       "19679   instead of politely knocking on the bathroom d...   \n",
       "136817  what 's better than double-fisting a newborn ?...   \n",
       "182687  the american people should elect gabe newell p...   \n",
       "353616  judge marilyn patel of the us district court f...   \n",
       "\n",
       "                                     tokens_with_stopword  \\\n",
       "156556         [what, tick, likes, to, run, ?, politicks]   \n",
       "19679   [instead, of, politely, knocking, on, the, bat...   \n",
       "136817  [what, 's, better, than, double-fisting, a, ne...   \n",
       "182687  [the, american, people, should, elect, gabe, n...   \n",
       "353616  [judge, marilyn, patel, of, the, us, district,...   \n",
       "\n",
       "        len_tokens_with_stopword  \\\n",
       "156556                         7   \n",
       "19679                         23   \n",
       "136817                        12   \n",
       "182687                        32   \n",
       "353616                        31   \n",
       "\n",
       "                                   Final_without_stopword  \\\n",
       "156556                         tick likes run ? politicks   \n",
       "19679   instead politely knocking bathroom door , kid ...   \n",
       "136817  's better double-fisting newborn ? hadouken ! ! !   \n",
       "182687  american people elect gabe newell president 20...   \n",
       "353616  judge marilyn patel us district court northern...   \n",
       "\n",
       "                                  tokens_without_stopword  \\\n",
       "156556                   [tick, likes, run, ?, politicks]   \n",
       "19679   [instead, politely, knocking, bathroom, door, ...   \n",
       "136817  ['s, better, double-fisting, newborn, ?, hadou...   \n",
       "182687  [american, people, elect, gabe, newell, presid...   \n",
       "353616  [judge, marilyn, patel, us, district, court, n...   \n",
       "\n",
       "        len_tokens_without_stopword  \n",
       "156556                            5  \n",
       "19679                            15  \n",
       "136817                            9  \n",
       "182687                           20  \n",
       "353616                           21  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test using other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 50)      20000050    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 199, 200)     20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 200)     30200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 200)     40200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 200)     50200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 195, 200)     60200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,329,436\n",
      "Trainable params: 329,386\n",
      "Non-trainable params: 20,000,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(os.path.join(model_save_path, model_name))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Clean_Punct</th>\n",
       "      <th>Final_with_stopword</th>\n",
       "      <th>tokens_with_stopword</th>\n",
       "      <th>len_tokens_with_stopword</th>\n",
       "      <th>Final_without_stopword</th>\n",
       "      <th>tokens_without_stopword</th>\n",
       "      <th>len_tokens_without_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>Merkel says supports some kind of no-fly zone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Merkel says supports some kind of nofly zone i...</td>\n",
       "      <td>merkel says supports some kind of nofly zone i...</td>\n",
       "      <td>[merkel, says, supports, some, kind, of, nofly...</td>\n",
       "      <td>10</td>\n",
       "      <td>merkel says supports kind nofly zone syria</td>\n",
       "      <td>[merkel, says, supports, kind, nofly, zone, sy...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17609</th>\n",
       "      <td>They have two children , Shane and Stephanie ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>They have two children   Shane and Stephanie  ...</td>\n",
       "      <td>they have two children shane and stephanie who...</td>\n",
       "      <td>[they, have, two, children, shane, and, stepha...</td>\n",
       "      <td>12</td>\n",
       "      <td>two children shane stephanie work wwe</td>\n",
       "      <td>[two, children, shane, stephanie, work, wwe]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6811</th>\n",
       "      <td>I've taken up speed reading. I can read 'War a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Ive taken up speed reading  I can read War and...</td>\n",
       "      <td>ive taken up speed reading i can read war and ...</td>\n",
       "      <td>[ive, taken, up, speed, reading, i, can, read,...</td>\n",
       "      <td>22</td>\n",
       "      <td>ive taken speed reading read war peace 20 seco...</td>\n",
       "      <td>[ive, taken, speed, reading, read, war, peace,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Knock, Knock. Who's there? Francis. Francis wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>Knock  Knock  Whos there  Francis  Francis who...</td>\n",
       "      <td>knock knock whos there francis francis who fra...</td>\n",
       "      <td>[knock, knock, whos, there, francis, francis, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>knock knock whos francis francis francis next ...</td>\n",
       "      <td>[knock, knock, whos, francis, francis, francis...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11765</th>\n",
       "      <td>A year in space: Scott Kelly and Mikhail Korni...</td>\n",
       "      <td>0</td>\n",
       "      <td>A year in space Scott Kelly and Mikhail Kornie...</td>\n",
       "      <td>a year in space scott kelly and mikhail kornie...</td>\n",
       "      <td>[a, year, in, space, scott, kelly, and, mikhai...</td>\n",
       "      <td>12</td>\n",
       "      <td>year space scott kelly mikhail kornienko retur...</td>\n",
       "      <td>[year, space, scott, kelly, mikhail, kornienko...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  label  \\\n",
       "16628  Merkel says supports some kind of no-fly zone ...      0   \n",
       "17609  They have two children , Shane and Stephanie ,...      0   \n",
       "6811   I've taken up speed reading. I can read 'War a...      1   \n",
       "1092   Knock, Knock. Who's there? Francis. Francis wh...      1   \n",
       "11765  A year in space: Scott Kelly and Mikhail Korni...      0   \n",
       "\n",
       "                                        Text_Clean_Punct  \\\n",
       "16628  Merkel says supports some kind of nofly zone i...   \n",
       "17609  They have two children   Shane and Stephanie  ...   \n",
       "6811   Ive taken up speed reading  I can read War and...   \n",
       "1092   Knock  Knock  Whos there  Francis  Francis who...   \n",
       "11765  A year in space Scott Kelly and Mikhail Kornie...   \n",
       "\n",
       "                                     Final_with_stopword  \\\n",
       "16628  merkel says supports some kind of nofly zone i...   \n",
       "17609  they have two children shane and stephanie who...   \n",
       "6811   ive taken up speed reading i can read war and ...   \n",
       "1092   knock knock whos there francis francis who fra...   \n",
       "11765  a year in space scott kelly and mikhail kornie...   \n",
       "\n",
       "                                    tokens_with_stopword  \\\n",
       "16628  [merkel, says, supports, some, kind, of, nofly...   \n",
       "17609  [they, have, two, children, shane, and, stepha...   \n",
       "6811   [ive, taken, up, speed, reading, i, can, read,...   \n",
       "1092   [knock, knock, whos, there, francis, francis, ...   \n",
       "11765  [a, year, in, space, scott, kelly, and, mikhai...   \n",
       "\n",
       "       len_tokens_with_stopword  \\\n",
       "16628                        10   \n",
       "17609                        12   \n",
       "6811                         22   \n",
       "1092                         11   \n",
       "11765                        12   \n",
       "\n",
       "                                  Final_without_stopword  \\\n",
       "16628         merkel says supports kind nofly zone syria   \n",
       "17609              two children shane stephanie work wwe   \n",
       "6811   ive taken speed reading read war peace 20 seco...   \n",
       "1092   knock knock whos francis francis francis next ...   \n",
       "11765  year space scott kelly mikhail kornienko retur...   \n",
       "\n",
       "                                 tokens_without_stopword  \\\n",
       "16628  [merkel, says, supports, kind, nofly, zone, sy...   \n",
       "17609       [two, children, shane, stephanie, work, wwe]   \n",
       "6811   [ive, taken, speed, reading, read, war, peace,...   \n",
       "1092   [knock, knock, whos, francis, francis, francis...   \n",
       "11765  [year, space, scott, kelly, mikhail, kornienko...   \n",
       "\n",
       "       len_tokens_without_stopword  \n",
       "16628                            7  \n",
       "17609                            6  \n",
       "6811                            12  \n",
       "1092                             8  \n",
       "11765                            8  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filename = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/dataset/final/shorttext.pickle\"\n",
    "data_out = pd.read_pickle(test_filename)\n",
    "data_out = data_out.sample(frac=1)\n",
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_pad_sequences() missing 1 required positional argument: 'max_sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c9407f0950dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2_data_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens_with_stopword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: create_pad_sequences() missing 1 required positional argument: 'max_sequence_length'"
     ]
    }
   ],
   "source": [
    "seq_test2, test2_data_padded = create_pad_sequences(data_out[\"tokens_with_stopword\"].tolist(), word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test2 = model.predict(test2_data_padded, batch_size=1024, verbose=0)\n",
    "\n",
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions_test2:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "sum(data_out.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinse part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tencent = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt\"\n",
    "embedding_ckpt_ch = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/tencent/var.ckpt\"\n",
    "embedding_json_ch = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/Tencent_AILab_ChineseEmbedding/tencent/var.json\"\n",
    "\n",
    "checkpoint_path_ch = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/ch_nlp/checkpoint/\"\n",
    "model_save_path_ch = \"/Users/rosalina_chen/Desktop/humor_recognition_ver3/model_created/ch_nlp/model_saved\"\n",
    "model_name_ch = \"chn_joke.h5\"\n",
    "\n",
    "embedding_dim_ch = 200\n",
    "max_sequence_len_ch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dadf3052bcf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mch_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtencent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_json_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_ckpt_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "ch_model = create_model(tencent, embedding_dim_ch, embedding_json_ch, embedding_ckpt_ch, max_sequence_len_ch, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67125</th>\n",
       "      <td>博物馆奇妙夜2          1.77                  1.8%   ...</td>\n",
       "      <td>0</td>\n",
       "      <td>博物馆奇妙夜2          177                  18      ...</td>\n",
       "      <td>[博物, 博物馆, 奇妙, 夜, 2, ,  , ,  , ,  , ,  , ,  , ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43595</th>\n",
       "      <td>8:女性的花季年龄上升为45岁。</td>\n",
       "      <td>1</td>\n",
       "      <td>8女性的花季年龄上升为45岁</td>\n",
       "      <td>[8, 女性, 的, 花季, 年龄, 上升, 升为, 45, 岁]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34629</th>\n",
       "      <td>老李：群狼可怕还是独狼可怕？ 老张：独狼吧，胃口小，吃到一半人没死，狼饱了。。。</td>\n",
       "      <td>1</td>\n",
       "      <td>老李群狼可怕还是独狼可怕 老张独狼吧胃口小吃到一半人没死狼饱了</td>\n",
       "      <td>[老李, 群, 狼, 可怕, 还是, 独, 狼, 可怕, ,  , , 老张, 独, 狼, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61016</th>\n",
       "      <td>曼联今年夏天以3075万英镑的高价引进了贝尔巴托夫；然后很可能会在1月份，以2200万英镑的...</td>\n",
       "      <td>0</td>\n",
       "      <td>曼联今年夏天以3075万英镑的高价引进了贝尔巴托夫然后很可能会在1月份以2200万英镑的高价...</td>\n",
       "      <td>[曼联, 今年, 今年夏天, 夏天, 以, 3075, 万英镑, 英镑, 的, 高价, 引进...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16589</th>\n",
       "      <td>希望你还保留一些厚颜,领着我看了一场又一场电影,就是不让我回家。</td>\n",
       "      <td>1</td>\n",
       "      <td>希望你还保留一些厚颜领着我看了一场又一场电影就是不让我回家</td>\n",
       "      <td>[希望, 你, 还, 保留, 一些, 厚颜, 领, 着, 我, 看, 了, 一场, 又, 一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29244</th>\n",
       "      <td>昨晚在火车上吃泡面“老坛酸菜面”吃得正香！后座的一个7、8岁的小男孩伸头过来，之后说了一句让...</td>\n",
       "      <td>1</td>\n",
       "      <td>昨晚在火车上吃泡面老坛酸菜面吃得正香后座的一个78岁的小男孩伸头过来之后说了一句让我吐血的话...</td>\n",
       "      <td>[昨晚, 在, 火车, 车上, 吃, 泡面, 老, 坛, 酸菜, 面, 吃, 得, 正, 香...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31041</th>\n",
       "      <td>大师兄~！二师兄被妖怪抓走了！</td>\n",
       "      <td>1</td>\n",
       "      <td>大师兄~二师兄被妖怪抓走了</td>\n",
       "      <td>[大师, 师兄, ~, 二, 师兄, 被, 妖怪, 抓走, 了]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108019</th>\n",
       "      <td>然而，在目前极端不稳定的市场状况下，任何单个事件都可能导致金融市场的波动。比如5月22日，西...</td>\n",
       "      <td>0</td>\n",
       "      <td>然而在目前极端不稳定的市场状况下任何单个事件都可能导致金融市场的波动比如5月22日西班牙央行...</td>\n",
       "      <td>[然而, 在, 目前, 极端, 不稳, 稳定, 的, 市场, 状况, 下任, 任何, 单个,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64640</th>\n",
       "      <td>比赛结束之后，天津队后防核心李玮锋也接受了采访，李玮锋首先表示，今天能够取得这场比赛的胜利还...</td>\n",
       "      <td>0</td>\n",
       "      <td>比赛结束之后天津队后防核心李玮锋也接受了采访李玮锋首先表示今天能够取得这场比赛的胜利还是相当...</td>\n",
       "      <td>[比赛, 结束, 之后, 后天, 天津, 天津队, 后防, 核心, 李玮锋, 也, 接受, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100358</th>\n",
       "      <td>这就意味着，陈晓、贝恩组合将在特别股东大会上直接掌握约12%的投票权，而黄光裕家族拥有的投票...</td>\n",
       "      <td>0</td>\n",
       "      <td>这就意味着陈晓贝恩组合将在特别股东大会上直接掌握约12的投票权而黄光裕家族拥有的投票权则将由...</td>\n",
       "      <td>[这, 就, 意味, 意味着, 陈, 晓, 贝, 恩, 组合, 将, 在, 特别, 股东, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111614 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentences  label  \\\n",
       "67125   博物馆奇妙夜2          1.77                  1.8%   ...      0   \n",
       "43595                                    8:女性的花季年龄上升为45岁。      1   \n",
       "34629            老李：群狼可怕还是独狼可怕？ 老张：独狼吧，胃口小，吃到一半人没死，狼饱了。。。      1   \n",
       "61016   曼联今年夏天以3075万英镑的高价引进了贝尔巴托夫；然后很可能会在1月份，以2200万英镑的...      0   \n",
       "16589                    希望你还保留一些厚颜,领着我看了一场又一场电影,就是不让我回家。      1   \n",
       "...                                                   ...    ...   \n",
       "29244   昨晚在火车上吃泡面“老坛酸菜面”吃得正香！后座的一个7、8岁的小男孩伸头过来，之后说了一句让...      1   \n",
       "31041                                     大师兄~！二师兄被妖怪抓走了！      1   \n",
       "108019  然而，在目前极端不稳定的市场状况下，任何单个事件都可能导致金融市场的波动。比如5月22日，西...      0   \n",
       "64640   比赛结束之后，天津队后防核心李玮锋也接受了采访，李玮锋首先表示，今天能够取得这场比赛的胜利还...      0   \n",
       "100358  这就意味着，陈晓、贝恩组合将在特别股东大会上直接掌握约12%的投票权，而黄光裕家族拥有的投票...      0   \n",
       "\n",
       "                                                  no_punc  \\\n",
       "67125   博物馆奇妙夜2          177                  18      ...   \n",
       "43595                                      8女性的花季年龄上升为45岁   \n",
       "34629                     老李群狼可怕还是独狼可怕 老张独狼吧胃口小吃到一半人没死狼饱了   \n",
       "61016   曼联今年夏天以3075万英镑的高价引进了贝尔巴托夫然后很可能会在1月份以2200万英镑的高价...   \n",
       "16589                       希望你还保留一些厚颜领着我看了一场又一场电影就是不让我回家   \n",
       "...                                                   ...   \n",
       "29244   昨晚在火车上吃泡面老坛酸菜面吃得正香后座的一个78岁的小男孩伸头过来之后说了一句让我吐血的话...   \n",
       "31041                                       大师兄~二师兄被妖怪抓走了   \n",
       "108019  然而在目前极端不稳定的市场状况下任何单个事件都可能导致金融市场的波动比如5月22日西班牙央行...   \n",
       "64640   比赛结束之后天津队后防核心李玮锋也接受了采访李玮锋首先表示今天能够取得这场比赛的胜利还是相当...   \n",
       "100358  这就意味着陈晓贝恩组合将在特别股东大会上直接掌握约12的投票权而黄光裕家族拥有的投票权则将由...   \n",
       "\n",
       "                                                   tokens  \n",
       "67125   [博物, 博物馆, 奇妙, 夜, 2, ,  , ,  , ,  , ,  , ,  , ,...  \n",
       "43595                   [8, 女性, 的, 花季, 年龄, 上升, 升为, 45, 岁]  \n",
       "34629   [老李, 群, 狼, 可怕, 还是, 独, 狼, 可怕, ,  , , 老张, 独, 狼, ...  \n",
       "61016   [曼联, 今年, 今年夏天, 夏天, 以, 3075, 万英镑, 英镑, 的, 高价, 引进...  \n",
       "16589   [希望, 你, 还, 保留, 一些, 厚颜, 领, 着, 我, 看, 了, 一场, 又, 一...  \n",
       "...                                                   ...  \n",
       "29244   [昨晚, 在, 火车, 车上, 吃, 泡面, 老, 坛, 酸菜, 面, 吃, 得, 正, 香...  \n",
       "31041                    [大师, 师兄, ~, 二, 师兄, 被, 妖怪, 抓走, 了]  \n",
       "108019  [然而, 在, 目前, 极端, 不稳, 稳定, 的, 市场, 状况, 下任, 任何, 单个,...  \n",
       "64640   [比赛, 结束, 之后, 后天, 天津, 天津队, 后防, 核心, 李玮锋, 也, 接受, ...  \n",
       "100358  [这, 就, 意味, 意味着, 陈, 晓, 贝, 恩, 组合, 将, 在, 特别, 股东, ...  \n",
       "\n",
       "[111614 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ch_dataset = pd.read_pickle(\"./data_created/chinese_dataset.pickle\")\n",
    "df_ch_dataset = df_ch_dataset.sample(frac=1)\n",
    "df_ch_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(df_ch_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from 'C:\\Users\\zhany\\Documents\\learning\\humor_recognition\\data\\emb_pretrained\\chn\\checpoint\\tencent\\var.json'.\n",
      "TF embeddings restored from 'C:\\Users\\zhany\\Documents\\learning\\humor_recognition\\data\\emb_pretrained\\chn\\checpoint\\tencent\\var.ckpt'.\n"
     ]
    }
   ],
   "source": [
    "word_to_index_ch = load_word_to_index(embedding_json_ch)\n",
    "#index_to_embedding_ch = load_embedding_tf(embedding_ckpt_ch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89291, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_train_ch, train_data_padded_ch = create_pad_sequences(train_dataset[\"tokens\"].tolist(), word_to_index_ch, max_sequence_len_ch)\n",
    "train_data_padded_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22323, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_test_ch, test_data_padded_ch = create_pad_sequences(test_dataset[\"tokens\"].tolist(), word_to_index_ch, max_sequence_len_ch)\n",
    "test_data_padded_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89291, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array_ch = [[1, 0]if v == 1 else [0, 1] for v in train_dataset['label'].values]\n",
    "label_array_ch = np.array(label_array_ch)\n",
    "label_array_ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80361 samples, validate on 8930 samples\n",
      "Epoch 1/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.4729 - acc: 0.7840 \n",
      "Epoch 00001: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1166s 15ms/sample - loss: 0.4714 - acc: 0.7847 - val_loss: 0.2245 - val_acc: 0.9137\n",
      "Epoch 2/10\n",
      "79872/80361 [============================>.] - ETA: 9s - loss: 0.1871 - acc: 0.9305 \n",
      "Epoch 00002: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1676s 21ms/sample - loss: 0.1868 - acc: 0.9307 - val_loss: 0.1442 - val_acc: 0.9434\n",
      "Epoch 3/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.1071 - acc: 0.9627 \n",
      "Epoch 00003: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1230s 15ms/sample - loss: 0.1071 - acc: 0.9627 - val_loss: 0.1078 - val_acc: 0.9598\n",
      "Epoch 4/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0571 - acc: 0.9827 \n",
      "Epoch 00004: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1098s 14ms/sample - loss: 0.0570 - acc: 0.9828 - val_loss: 0.1055 - val_acc: 0.9600\n",
      "Epoch 5/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0357 - acc: 0.9895 \n",
      "Epoch 00005: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1108s 14ms/sample - loss: 0.0356 - acc: 0.9895 - val_loss: 0.1180 - val_acc: 0.9597\n",
      "Epoch 6/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0184 - acc: 0.9955 \n",
      "Epoch 00006: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1169s 15ms/sample - loss: 0.0183 - acc: 0.9956 - val_loss: 0.1038 - val_acc: 0.9641\n",
      "Epoch 7/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0110 - acc: 0.9977 \n",
      "Epoch 00007: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1200s 15ms/sample - loss: 0.0110 - acc: 0.9977 - val_loss: 0.1168 - val_acc: 0.9644\n",
      "Epoch 8/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0066 - acc: 0.9989 \n",
      "Epoch 00008: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1153s 14ms/sample - loss: 0.0066 - acc: 0.9990 - val_loss: 0.1178 - val_acc: 0.9656\n",
      "Epoch 9/10\n",
      "79872/80361 [============================>.] - ETA: 6s - loss: 0.0046 - acc: 0.9992 \n",
      "Epoch 00009: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1159s 14ms/sample - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1161 - val_acc: 0.9660\n",
      "Epoch 10/10\n",
      "79872/80361 [============================>.] - ETA: 5s - loss: 0.0045 - acc: 0.9992 \n",
      "Epoch 00010: saving model to ./model_created/ch_nlp/checkpoint/\n",
      "80361/80361 [==============================] - 1057s 13ms/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1163 - val_acc: 0.9673\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(checkpoint_path_ch):\n",
    "    os.makedirs(checkpoint_path_ch)\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_ch, save_weights_only=True, verbose=1)\n",
    "hist = ch_model.fit(train_data_padded_ch, label_array_ch, epochs=num_epoches, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_save_path_ch):\n",
    "    os.makedirs(model_save_path_ch)\n",
    "    \n",
    "ch_model.save(os.path.join(model_save_path_ch, model_name_ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d62d463e213f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_padded_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ch_model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = ch_model.predict(test_data_padded_ch, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9657304125789544"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "sum(test_dataset.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict the eastern or/and western joke (DA QUI POSSO LANCIARE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_translate_eng_cn(sentence):\n",
    "    url = 'http://api.fanyi.baidu.com/api/trans/vip/translate'\n",
    "\n",
    "    # non mostrare appid e key agli altri\n",
    "    appid = '20200329000408142'\n",
    "    key = 'riUpq41_ifBFCFB5c6NF'\n",
    "\n",
    "    salt = '12345654321234'   #  random number\n",
    "    \n",
    "    sign = hashlib.md5((appid + sentence + salt + key).encode('UTF-8')).hexdigest()\n",
    "    \n",
    "    en_form_data = {\n",
    "    'q': sentence,         \n",
    "    'from': 'auto',    \n",
    "    'to': \"en\",   \n",
    "    'appid': appid,\n",
    "    'salt': salt,\n",
    "    'sign': sign\n",
    "    }\n",
    "    en_request = requests.get(url, params=en_form_data)\n",
    "    en_sentence = en_request.json()['trans_result'][0]['dst']\n",
    "    \n",
    "    time.sleep(1)  # pausa 1 sec, standard baidu api limitation\n",
    "    \n",
    "    cn_form_data = {\n",
    "        'q': sentence,\n",
    "        'from': 'auto',\n",
    "        'to': 'zh',\n",
    "        'appid': appid,\n",
    "        'salt': salt,\n",
    "        'sign': sign\n",
    "    }\n",
    "    cn_request = requests.get(url, params=cn_form_data)\n",
    "    cn_sentence = cn_request.json()['trans_result'][0]['dst']\n",
    "    \n",
    "    return en_sentence, cn_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Try Baidu translation', '试试百度翻译')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baidu_translate_eng_cn(\"试试百度翻译\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_translate_eng_cn(sentence):\n",
    "    \"\"\"\n",
    "    return the englesh and chinese sentence by using google translate\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    eng_sentence = translator.translate(sentence, dest=\"en\").text\n",
    "    cn_sentence = translator.translate(sentence, dest=\"zh-cn\").text\n",
    "    return eng_sentence, cn_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('try google translator', '尝试谷歌翻译')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_translate_eng_cn(\"try google translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_word_eng(sentence):\n",
    "    \"\"\"\n",
    "    tokenizing the input sentence, include also remove punctuation, lower casing\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return [word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentence, model, word_to_index, max_seq_length):\n",
    "    _, sentence_padded = create_pad_sequences(tokenizing_word_eng(sentence), word_to_index, max_seq_length)\n",
    "    result = model.predict(sentence_padded)\n",
    "    \n",
    "    label = [1, 0]\n",
    "    prediction = label[np.argmax(result)]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(translator=\"google\"):\n",
    "    \n",
    "    en_word_json_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json\"\n",
    "    cn_word_json_path = r\"/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/cn_var.json\"\n",
    "    en_model_path = \"../model/modelli_creati/eng_joke.h5\"\n",
    "    cn_model_path = \"../model/modelli_creati/chn_joke.h5\"\n",
    "\n",
    "    en_seq_len = 200\n",
    "    cn_seq_len = 200\n",
    "    \n",
    "    en_word_to_index = load_word_to_index(en_word_json_path)\n",
    "   # cn_word_to_index = load_word_to_index(cn_word_json_path)\n",
    "    \n",
    "    en_model = tf.keras.models.load_model(en_model_path)\n",
    "   # cn_model = tf.keras.models.load_model(cn_model_path)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            sentence = str(input(\"Please insert your sentence: \"))\n",
    "            if sentence.lower().strip() == \"end\":\n",
    "                break\n",
    "            if translator.lower().strip() == \"google\":\n",
    "                en_sentence, cn_sentence = google_translate_eng_cn(sentence)\n",
    "            elif translator.lower().strip() == \"baidu\":\n",
    "                en_sentence, cn_sentence = baidu_translate_eng_cn(sentence)\n",
    "            else:\n",
    "                print(\"there are not translator requested\")\n",
    "            en_result = prediction(en_sentence, en_model, en_word_to_index, en_seq_len)\n",
    "           # cn_result = prediction(cn_sentence, cn_model, cn_word_to_index,cn_seq_len)\n",
    "            \n",
    "            if en_result == 1:\n",
    "                print(\"this is an english joke\")\n",
    "            else:\n",
    "                print(\"this is not an english joke\")\n",
    "          #  if cn_result == 1:\n",
    "           #     print(\"this is a chinese joke\")\n",
    "           # else:\n",
    "          #      print(\"this is not a chinese joke\")\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from '/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'.\n",
      "Please insert your sentence: hi how are you\n",
      "this is an english joke\n",
      "Please insert your sentence: My mother-in-law fell down a wishing well. I was amazed – I never knew they worked\n",
      "this is an english joke\n",
      "Please insert your sentence: i am 23 years old\n",
      "this is an english joke\n",
      "Please insert your sentence:  MediaWiki helps you collect and organize knowledge and make it available to people.\n",
      "this is an english joke\n",
      "Please insert your sentence: God wants spiritual fruit, not religious nuts.\n",
      "this is an english joke\n",
      "Please insert your sentence: That day, when they came back from school, their own son said\n",
      "this is not an english joke\n"
     ]
    }
   ],
   "source": [
    "main(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2c223e39d42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#funzione per testare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-87d519270f40>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(translator)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcn_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0men_word_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_word_json_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0;31m# cn_word_to_index = load_word_to_index(cn_word_json_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-735cf9ef291b>\u001b[0m in \u001b[0;36mload_word_to_index\u001b[0;34m(dict_word_index_filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdict_word_index_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindex\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_word_index_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_LAST_INDEX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/rosalina_chen/Desktop/humor_recognition_ver3/root/model/dizionari/en_var.json'"
     ]
    }
   ],
   "source": [
    "main(\"google\") #funzione per testare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
